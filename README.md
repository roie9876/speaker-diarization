# Speaker Diarization & Selective Transcription System

ğŸ¤ **AI-powered voice identification and selective transcription for meetings and conversations**

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Production_Ready-success.svg)](docs/IMPLEMENTATION_STATUS.md)

---

## ğŸ¯ What Does This System Do?

This application solves a common problem: **extracting what ONE specific person said in a group conversation** without manually listening to hours of audio.

### Real-World Use Cases

- ğŸ“ **Meeting Minutes**: Extract only your manager's decisions from a 2-hour meeting
- ğŸ“ **Student Notes**: Transcribe only the professor's words, not student questions
- ï¿½ï¸ **Interview Processing**: Get just the interviewee's responses, not the interviewer
- ğŸ“ **Call Analysis**: Isolate customer statements from agent responses
- ğŸ¢ **Conference Sessions**: Capture specific speaker's presentations from panel discussions

### How It Works (In Simple Terms)

1. **ğŸ‘¤ Training Phase** - Upload 30-60 seconds of the target person speaking alone
   - System learns their unique voice characteristics (like a voice fingerprint)
   - Creates a speaker profile you can reuse

2. **ğŸ” Identification Phase** - Upload meeting recordings (or use live microphone)
   - Detects who is speaking at each moment (Speaker A, B, C...)
   - Compares each speaker's voice to your saved profile
   - Identifies segments where the target person is talking

3. **ğŸ“ Transcription Phase** - Converts speech to text (only for target speaker)
   - Skips everyone else
   - Returns timestamped transcript: `[00:12:34] What the person said...`

---

## ğŸ’¡ Why Use This?

### The Problem
You have a 2-hour meeting recording with 5 people talking. You only need what **one specific person** said, but:
- âŒ Manual listening takes 2+ hours
- âŒ Generic transcription gives you everyone's words (mixed together)
- âŒ No easy way to filter by speaker

### The Solution
This system:
- âœ… Learns the target person's voice once (30-60 seconds)
- âœ… Automatically identifies when they speak in any recording
- âœ… Transcribes ONLY their words with timestamps
- âœ… Works in real-time or processes existing files

### Example Result
**Input**: 2-hour meeting with 5 speakers (120 minutes of audio)  
**Output**: 15-minute transcript of just the CEO's comments

```
[00:03:15] CEO: I think we should move forward with the merger.
[00:12:42] CEO: The Q3 targets are ambitious but achievable.
[00:45:18] CEO: Let's allocate more budget to marketing.
[01:23:09] CEO: I approve this plan. Let's execute next week.
```

**Time saved**: 1 hour 45 minutes ğŸ‰

---

## ğŸ¥ Demo & Screenshots

### Live Monitoring Interface
Real-time speaker detection with live audio waveform, transcript display, and session statistics.

*(Screenshots: Add your UI screenshots to `docs/images/` folder)*

### Workflow Overview
1. **Enrollment Tab** â†’ Upload 30s voice sample â†’ Create profile
2. **Batch Tab** â†’ Upload meeting files â†’ Process â†’ Download transcripts
3. **Live Tab** â†’ Start monitoring â†’ Speak â†’ See transcripts in real-time

**ğŸ“– See**: [Waveform Visualization](./docs/WAVEFORM_VISUALIZATION.md) for technical details

---

## âš¡ Quick Start

### Prerequisites
- **Python 3.10+** (3.10, 3.11, or 3.12 recommended)
- **Azure Speech Service** account ([Free tier available](https://azure.microsoft.com/en-us/free/ai-services/))
- **Hugging Face** account (free) for pyannote.audio models
- **8GB+ RAM** recommended
- **GPU optional** (Apple Silicon MPS, NVIDIA CUDA, or CPU mode)

### Installation

```bash
# Clone repository
cd speaker-diarization

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your Azure and Hugging Face credentials

# Run application
streamlit run src/ui/app.py
```

### First Time Setup

1. **Azure Speech Service**:
   - Create Azure account: https://azure.microsoft.com/
   - Create Speech Service resource
   - Copy API key and region to `.env`

2. **Hugging Face** (for pyannote models):
   - Create account: https://huggingface.co/
   - Accept user agreements:
     - https://huggingface.co/pyannote/speaker-diarization-3.1
     - https://huggingface.co/pyannote/segmentation-3.0
   - Generate access token: https://huggingface.co/settings/tokens
   - Add token to `.env`

---

## ğŸš€ Features & Capabilities

### 1. ğŸ‘¤ Speaker Enrollment (Voice Profile Creation)
- **Upload Reference Audio**: 30-60 seconds of the target person speaking alone
- **Voice Fingerprint Extraction**: System creates a unique 512-dimensional embedding
- **Profile Management**: Save, load, delete speaker profiles
- **Multi-Profile Support**: Create profiles for different people
- **Quality Validation**: Automatic audio quality checks

**ğŸ“– See**: [Enrollment Requirements](./docs/requirements/functional-requirements.md#enrollment-mode)

---

### 2. ğŸ“ Batch Processing (Process Recordings)
- **Multi-File Upload**: Process multiple recordings simultaneously
- **Flexible Formats**: MP3, WAV, M4A, FLAC supported
- **Adjustable Thresholds**: Fine-tune speaker matching sensitivity
- **Progress Tracking**: Real-time processing status per file
- **Detailed Results**: 
  - Timestamped transcripts with confidence scores
  - Speaker match statistics
  - Audio segment metadata
- **Export Options**: JSON, TXT, CSV formats

**ğŸ“– See**: [Batch Processing Guide](./docs/requirements/functional-requirements.md#batch-processing-mode)

---

### 3. ğŸ”´ Live Monitoring (Real-Time Transcription) âš¡ NEW: Push Stream Technology
- **Real-Time Audio Capture**: Direct microphone input with WebSocket streaming
- **Live Speaker Detection**: Identify target speaker as they speak
- **Ultra-Low Latency**: **1-2 second response time** (Azure Push Stream API)
- **Studio-Quality Accuracy**: **90-95% Hebrew transcription** (matching Azure Speech Studio)
- **Visual Feedback**:
  - Live audio waveform display
  - Audio level meter with voice activity detection
  - Target speaker detection indicators
  - Real-time confidence scores
- **Session Management**: Save and export live session transcripts
- **Multi-Language Support**: Hebrew, English, 100+ languages
- **No File I/O**: Direct memory streaming for maximum reliability

**ğŸ“– See**: [Live Monitoring Guide](./docs/requirements/functional-requirements.md#live-monitoring-mode)  
**ğŸš€ Technical Details**: [Push Stream Implementation](./docs/fixes/PUSH_STREAM_IMPLEMENTATION.md)

---

### Additional Features
- âœ… **GPU Acceleration**: MPS (Apple Silicon), CUDA (NVIDIA), CPU fallback
- âœ… **Multi-Language**: Support for 100+ languages via Azure Speech
- âœ… **High Accuracy**: 90%+ speaker identification, **90-95% transcription accuracy**
- âœ… **Privacy Options**: On-premises deployment available (Azure container)
- âœ… **Confidence Scores**: Know how reliable each transcription is
- âœ… **Audio Visualization**: Real-time waveform and level meters
- âœ… **Streaming Transcription**: WebSocket-based real-time STT (Azure Push Stream)
- âœ… **Ultra-Low Latency**: 1-2 second response time in live mode

**ğŸ“– Full specs**: [Functional Requirements](./docs/requirements/functional-requirements.md) (51 features)

---

## ğŸ“š Documentation

Comprehensive documentation is available in the [`docs/`](./docs/) folder:

### ğŸš€ Getting Started
- **[Quick Start Guide](./docs/QUICK_START.md)** - Installation and first-run walkthrough
- **[Setup Complete](./docs/SETUP_COMPLETE.md)** - Verify your installation
- **[Implementation Status](./docs/IMPLEMENTATION_STATUS.md)** - Current project status

### ğŸ“– Project Documentation
- **[Project Overview](./docs/PROJECT_OVERVIEW.md)** - Goals, scope, and business context
- **[Project Organization](./docs/PROJECT_ORGANIZATION.md)** - How the codebase is structured
- **[Changelog](./docs/CHANGELOG.md)** - Version history and updates

### ğŸ—ï¸ Architecture & Technical
- **[System Architecture](./docs/architecture/system-architecture.md)** - Technical design and component interactions
- **[Technology Stack](./docs/technical/technology-stack.md)** - Libraries, dependencies, and setup guides
- **[pyannote Integration](./docs/technical/pyannote-integration.md)** - Deep dive into speaker diarization
- **[Waveform Visualization](./docs/WAVEFORM_VISUALIZATION.md)** - Real-time audio display implementation

### ğŸ‘¨â€ğŸ’» Development
- **[Implementation Guide](./docs/IMPLEMENTATION_GUIDE.md)** - Step-by-step development roadmap (4 phases)
- **[Functional Requirements](./docs/requirements/functional-requirements.md)** - 51 detailed feature specifications
- **[Contributing Guide](./docs/CONTRIBUTING.md)** - How to contribute to the project
- **[GitHub Copilot Instructions](./.github/copilot-instructions.md)** - AI-assisted development guide

### âœ… Quality Assurance
- **[Pre-Push Checklist](./docs/PRE_PUSH_CHECKLIST.md)** - Code quality checks before commits
- **[Ready to Push](./docs/READY_TO_PUSH.md)** - Deployment readiness verification

---

## ğŸ—ï¸ Technology Stack

| Component | Technology |
|-----------|-----------|
| **Language** | Python 3.10+ |
| **UI Framework** | Streamlit |
| **Speaker Diarization** | pyannote.audio 3.1+ |
| **Speech-to-Text** | Azure Cognitive Services (Push Stream API) |
| **Real-Time Streaming** | Azure Speech SDK WebSocket |
| **ML Framework** | PyTorch (MPS/CUDA/CPU) |
| **Audio Processing** | librosa, soundfile, pyaudio |

---

## ğŸ“ Project Structure

```
speaker-diarization/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ copilot-instructions.md     # GitHub Copilot implementation guide
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ services/                   # Core processing services
â”‚   â”‚   â”œâ”€â”€ streaming_transcription_service.py  # âš¡ WebSocket streaming STT
â”‚   â”‚   â”œâ”€â”€ diarization_service.py             # Speaker detection
â”‚   â”‚   â”œâ”€â”€ identification_service.py          # Voice matching
â”‚   â”‚   â”œâ”€â”€ transcription_service.py           # Batch STT (fallback)
â”‚   â”‚   â””â”€â”€ profile_manager.py                 # Profile CRUD
â”‚   â”œâ”€â”€ processors/                 # Batch and real-time processors
â”‚   â”œâ”€â”€ ui/                         # Streamlit interface
â”‚   â”œâ”€â”€ config/                     # Configuration management
â”‚   â””â”€â”€ utils/                      # Helper utilities
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ profiles/                   # Speaker profiles
â”‚   â”œâ”€â”€ results/                    # Processing results
â”‚   â””â”€â”€ temp/                       # Temporary files
â”œâ”€â”€ docs/                           # Comprehensive documentation
â”œâ”€â”€ tests/                          # Unit and integration tests
â”œâ”€â”€ .env                            # Environment variables (create from .env.example)
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # This file
```

---

## ğŸ”§ Configuration

Create `.env` file with:

```bash
# Azure Speech Service
AZURE_SPEECH_KEY=your_azure_key_here
AZURE_REGION=eastus
AZURE_MODE=cloud  # or 'container' for on-premises

# Hugging Face (for pyannote models)
HUGGING_FACE_HUB_TOKEN=your_hf_token_here

# Processing Settings
SIMILARITY_THRESHOLD=0.75
USE_GPU=true
```

---

## ğŸ¯ How It Works (Technical Pipeline)

The system combines three AI technologies to achieve selective transcription:

### Step-by-Step Processing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Meeting Recording (MP3/WAV) or Live Microphone      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Speaker Diarization (pyannote.audio)               â”‚
â”‚  "Who spoke when?"                                           â”‚
â”‚  Output: Segments with speaker labels                       â”‚
â”‚    â€¢ [0.0s - 3.5s] SPEAKER_00                              â”‚
â”‚    â€¢ [3.8s - 7.2s] SPEAKER_01                              â”‚
â”‚    â€¢ [7.5s - 12.1s] SPEAKER_00                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Voice Identification (pyannote embedding)          â”‚
â”‚  "Which speaker is our target person?"                       â”‚
â”‚  â€¢ Extract 512-D embedding for each segment                 â”‚
â”‚  â€¢ Compare with saved profile embedding                     â”‚
â”‚  â€¢ Calculate cosine similarity                              â”‚
â”‚  â€¢ If similarity â‰¥ 0.50 â†’ TARGET MATCH âœ…                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Streaming Transcription (Azure Push Stream) âš¡     â”‚
â”‚  "Convert only target person's speech to text"              â”‚
â”‚  â€¢ Stream matched segments via WebSocket to Azure           â”‚
â”‚  â€¢ Real-time continuous recognition (1-2s latency)          â”‚
â”‚  â€¢ Support for Hebrew, English, 100+ languages              â”‚
â”‚  â€¢ Return confidence scores + event-based callbacks         â”‚
â”‚  â€¢ No file I/O (direct memory streaming)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OUTPUT: Timestamped Transcript (JSON/TXT)                  â”‚
â”‚  [00:01:23] This is what I said during the meeting.         â”‚
â”‚  [00:02:45] I think we should proceed with option A.        â”‚
â”‚  [00:05:12] Let me clarify my earlier point about the budgetâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Technologies

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Speaker Diarization** | [pyannote.audio 3.1+](https://github.com/pyannote/pyannote-audio) | Detects "who spoke when" |
| **Voice Embeddings** | pyannote embedding model | Extracts voice fingerprints (512-D vectors) |
| **Speech-to-Text** | [Azure Speech SDK](https://azure.microsoft.com/en-us/products/ai-services/speech-to-text) (Push Stream API) | Converts speech to text (100+ languages, WebSocket streaming) |
| **Real-Time Streaming** | Azure PushAudioInputStream | Direct memory-to-cloud audio streaming |
| **ML Backend** | PyTorch (MPS/CUDA/CPU) | Neural network inference |

**ğŸ“– Deep dive**: See [pyannote Integration Guide](./docs/technical/pyannote-integration.md) for technical details

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest --cov=src tests/

# Run specific test file
pytest tests/test_services.py
```

---

## ğŸš¢ Deployment

### Development (Cloud Azure)
- Use Azure Speech Service cloud endpoint
- Fast setup, pay-per-use

### Production (On-Premises Container)
- Deploy Azure Speech Service as Docker container
- Data stays on local network
- One-time licensing fee

See [deployment documentation](./docs/architecture/system-architecture.md#deployment-architecture) for details.

---

## ğŸ¤ Contributing

1. Read [IMPLEMENTATION_GUIDE.md](./docs/IMPLEMENTATION_GUIDE.md)
2. Follow the phased development approach
3. Use [copilot-instructions.md](./.github/copilot-instructions.md) for implementation context
4. Write tests for new features
5. Update documentation as needed

---

## ğŸ“Š Performance

| Mode | Processing Speed | Accuracy | GPU Required? |
|------|-----------------|----------|---------------|
| **Enrollment** | 5-15 seconds | N/A | Optional |
| **Batch** | 1-2x real-time | 90-95% | Recommended |
| **Live (Push Stream)** | **1-2s latency** âš¡ | **90-95%** ğŸ¯ | **Required** |

**Performance Improvements (v2.0)**:
- âœ… Latency reduced from 5-8s â†’ 1-2s (75% improvement)
- âœ… Hebrew accuracy improved from 60-70% â†’ 90-95% (30% improvement)
- âœ… No file I/O errors or race conditions
- âœ… WebSocket streaming matches Azure Speech Studio quality

**Tested on**: Apple M1 Max (excellent performance with MPS)

---

## ğŸ› Troubleshooting

### Common Issues

**PyAudio installation fails**:
```bash
# macOS
brew install portaudio
pip install pyaudio

# Ubuntu
sudo apt-get install portaudio19-dev
pip install pyaudio
```

**pyannote models won't download**:
- Verify Hugging Face token is set
- Accept user agreements on model pages
- Check internet connection

**Low speaker identification accuracy**:
- Use 30+ seconds of reference audio
- Ensure reference audio is clear
- Try adjusting similarity threshold
- Use multiple reference samples

See [Technology Stack](./docs/technical/technology-stack.md#troubleshooting) for more solutions.

---

## ğŸ“„ License

[Add your license here]

---

## ğŸ‘¥ Team

[Add team/contact information]

---

## ğŸ™ Acknowledgments

- **pyannote.audio** - State-of-the-art speaker diarization
- **Azure Cognitive Services** - Speech-to-text transcription  
- **Streamlit** - Rapid UI development

---

## ğŸ”— Key Documentation Links

### For Users
- ğŸ“– **[Quick Start Guide](./docs/QUICK_START.md)** - Get started in 10 minutes
- ğŸ“– **[Setup Complete Checklist](./docs/SETUP_COMPLETE.md)** - Verify installation
- ï¿½ **[Troubleshooting](#-troubleshooting)** - Common issues and fixes (see below)

### For Developers
- ğŸ—ï¸ **[System Architecture](./docs/architecture/system-architecture.md)** - How components interact
- ğŸ“ **[Implementation Guide](./docs/IMPLEMENTATION_GUIDE.md)** - 4-phase development roadmap
- ğŸ¤– **[GitHub Copilot Instructions](./.github/copilot-instructions.md)** - AI-assisted coding guide
- ğŸ”§ **[Technology Stack](./docs/technical/technology-stack.md)** - All dependencies and setup
- ğŸ¯ **[Functional Requirements](./docs/requirements/functional-requirements.md)** - 51 feature specs
- ğŸ› **[Bug Fixes Documentation](./docs/fixes/README.md)** - All fixes and improvements

### For Contributors
- ğŸ¤ **[Contributing Guide](./docs/CONTRIBUTING.md)** - How to contribute
- âœ… **[Pre-Push Checklist](./docs/PRE_PUSH_CHECKLIST.md)** - Quality checks
- ğŸ“‹ **[Implementation Status](./docs/IMPLEMENTATION_STATUS.md)** - Current state
- ğŸ“ **[Changelog](./docs/CHANGELOG.md)** - Version history

---

## ğŸ”§ Troubleshooting

### Batch Mode: No Transcripts in Multi-Speaker Audio

**Symptoms**:
- âœ… Single speaker files work fine
- âŒ Multi-speaker files return 0 transcripts
- Logs show all segments with same similarity (e.g., 0.2300)

**Cause**: pyannote caching bug (fixed in v2.0.1)

**Solution**: Update to v2.0.1 or later. The fix manually extracts audio segments before embedding inference.

**Verification**:
```bash
# Check your version
grep "Version" docs/IMPLEMENTATION_STATUS.md
# Should show: Version: 2.0.1 or later
```

---

### Live Mode: Transcripts Missing After Stopping

**Symptoms**:
- Speak during monitoring
- Click "Stop Monitoring"
- No transcripts appear

**Cause**: Azure responses delayed 2-5s, UI stopped checking queue (fixed in v2.0.1)

**Solution**:
1. Update to v2.0.1 or later
2. After clicking "Stop", wait 5 seconds for delayed transcripts

---

### Similarity Scores Too Low (< 0.40)

**Symptoms**:
- Your voice not detected (similarity 0.35-0.45)
- "No target speaker detected" messages
- Works better in single-speaker scenarios

**Cause**: Speaker profile quality or threshold too strict

**Solutions**:

**Option 1 - Lower Threshold** (Quick fix):
```bash
# Edit .env file
SIMILARITY_THRESHOLD=0.35  # Lower from 0.40
```

**Option 2 - Recreate Profile** (Best fix):
1. Go to Enrollment Tab
2. Delete current profile
3. Record 45-60 seconds of clear, varied speech
4. Create new profile
5. Target similarity: 0.60+ (much more robust)

---

### Hebrew Transcription Accuracy Low

**Symptoms**:
- Transcripts showing gibberish or wrong words
- Accuracy < 80%

**Cause**: Wrong language or old version

**Solutions**:
1. **Check Language Setting**: Ensure `he-IL` (Hebrew Israel) is selected
2. **Update to v2.0+**: Push Stream implementation improved Hebrew from 60-70% â†’ 90-95%
3. **Verify Azure Region**: Use `eastus` or `westeurope` (better Hebrew support)

---

### Pyannote Model Download Issues

**Symptoms**:
- "Failed to load embedding model"
- Authentication errors

**Solutions**:
1. Accept user agreements:
   - https://huggingface.co/pyannote/speaker-diarization-3.1
   - https://huggingface.co/pyannote/segmentation-3.0
   - https://huggingface.co/pyannote/embedding
2. Generate token: https://huggingface.co/settings/tokens
3. Update `.env`:
   ```bash
   HUGGING_FACE_HUB_TOKEN=your_token_here
   ```

---

### GPU Not Being Used (Slow Processing)

**Symptoms**:
- Processing takes >10 seconds per file
- Logs show "using device: cpu"

**Solutions**:

**For Apple Silicon (M1/M2/M3)**:
```bash
# Check MPS availability
python3 -c "import torch; print(torch.backends.mps.is_available())"
# Should print: True
```

**For NVIDIA GPU**:
```bash
# Check CUDA
python3 -c "import torch; print(torch.cuda.is_available())"
# Should print: True
```

**Enable GPU in .env**:
```bash
USE_GPU=true
```

---

**Still having issues?** Check the full documentation:
- ğŸ“š [Technology Stack](./docs/technical/technology-stack.md#troubleshooting)
- ğŸ› [Bug Fixes](./docs/fixes/README.md)
- ğŸ“‹ [Implementation Status](./docs/IMPLEMENTATION_STATUS.md)

---

**Project Status**: âœ… **Production Ready** - All critical bugs fixed  
**Last Updated**: October 27, 2025  
**Version**: 2.0.1  
**Maintained by**: [Add your name/organization]

---

## ğŸ“‚ Project Structure

```
speaker-diarization/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ copilot-instructions.md     # ğŸ¤– AI assistant implementation guide
â”œâ”€â”€ src/                            # ğŸ’» Source Code
â”‚   â”œâ”€â”€ services/                   # Core AI services
â”‚   â”‚   â”œâ”€â”€ diarization_service.py  # Speaker detection
â”‚   â”‚   â”œâ”€â”€ identification_service.py # Voice matching
â”‚   â”‚   â”œâ”€â”€ transcription_service.py # Speech-to-text
â”‚   â”‚   â””â”€â”€ profile_manager.py      # Speaker profile CRUD
â”‚   â”œâ”€â”€ processors/                 # Processing orchestration
â”‚   â”‚   â”œâ”€â”€ batch_processor.py      # Multi-file processing
â”‚   â”‚   â””â”€â”€ realtime_processor.py   # Live audio streaming
â”‚   â”œâ”€â”€ ui/                         # ğŸ–¥ï¸ Streamlit Interface
â”‚   â”‚   â”œâ”€â”€ app.py                  # Main application
â”‚   â”‚   â”œâ”€â”€ enrollment_tab.py       # Voice profile creation
â”‚   â”‚   â”œâ”€â”€ batch_tab.py           # Batch processing UI
â”‚   â”‚   â””â”€â”€ live_tab.py            # Real-time monitoring
â”‚   â”œâ”€â”€ config/                     # Configuration management
â”‚   â”‚   â””â”€â”€ config_manager.py
â”‚   â””â”€â”€ utils/                      # Helper utilities
â”‚       â”œâ”€â”€ audio_utils.py          # Audio format conversion
â”‚       â””â”€â”€ logger.py               # Logging setup
â”œâ”€â”€ docs/                           # ğŸ“š Documentation
â”‚   â”œâ”€â”€ architecture/               # System design
â”‚   â”œâ”€â”€ requirements/               # Feature specifications
â”‚   â”œâ”€â”€ technical/                  # Tech deep-dives
â”‚   â”œâ”€â”€ PROJECT_OVERVIEW.md
â”‚   â”œâ”€â”€ IMPLEMENTATION_GUIDE.md
â”‚   â”œâ”€â”€ QUICK_START.md
â”‚   â””â”€â”€ [20+ more docs...]
â”œâ”€â”€ data/                           # ğŸ’¾ Data Storage
â”‚   â”œâ”€â”€ profiles/                   # Speaker profiles (JSON)
â”‚   â”œâ”€â”€ results/                    # Transcripts output
â”‚   â””â”€â”€ temp/                       # Processing temp files
â”œâ”€â”€ tests/                          # âœ… Testing
â”‚   â”œâ”€â”€ test_services.py
â”‚   â”œâ”€â”€ test_processors.py
â”‚   â””â”€â”€ verify_installation.py
â”œâ”€â”€ .env                            # ğŸ” Environment variables (create from .env.example)
â”œâ”€â”€ requirements.txt                # ğŸ“¦ Python dependencies
â””â”€â”€ README.md                       # ğŸ“– This file
```

**ğŸ“– Full tree**: [docs/PROJECT_STRUCTURE.txt](./docs/PROJECT_STRUCTURE.txt)  
**ğŸ“– Organization**: [docs/PROJECT_ORGANIZATION.md](./docs/PROJECT_ORGANIZATION.md)
